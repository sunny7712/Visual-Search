{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\Desktop\\Home\\Projects\\Python\\Visual Search\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from datasets import Dataset, DatasetDict, Image, Features, ClassLabel, Value, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\Desktop\\Home\\Projects\\Python\\Visual Search\\venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at vit-base-fashion and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = 'vit-base-fashion'\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\i'\n",
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_33688\\1968970228.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv('fashion-dataset\\styles.csv', on_bad_lines='skip')\n",
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_33688\\1968970228.py:5: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  image_dir = 'fashion-dataset\\images'\n",
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_33688\\1968970228.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['image_path'] = sub_df['id'].apply(lambda x: os.path.join(image_dir, str(x) + '.jpg'))\n",
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_33688\\1968970228.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exact_df['id'] = exact_df['id'].apply(lambda x : str(x))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fashion-dataset\\styles.csv', on_bad_lines='skip')\n",
    "freq = df['subCategory'].value_counts()\n",
    "top_freq = {category: freq.loc[category] for category in list(freq.index) if freq.loc[category] >= 100}\n",
    "sub_df = df[df['subCategory'].isin(list(top_freq.keys()))]\n",
    "image_dir = 'fashion-dataset\\images'\n",
    "sub_df['image_path'] = sub_df['id'].apply(lambda x: os.path.join(image_dir, str(x) + '.jpg'))\n",
    "exact_df = sub_df[['image_path', 'subCategory', 'id']]\n",
    "exact_df['id'] = exact_df['id'].apply(lambda x : str(x))\n",
    "exact_df = exact_df.rename(columns={'image_path': 'image',\n",
    "                   'subCategory': 'labels'})\n",
    "ls1 = set(exact_df['id'])\n",
    "ls2 = set(os.listdir(image_dir))\n",
    "not_available_imgs = [i for i in ls1 if f\"{i}.jpg\" not in ls2]\n",
    "exact_df = exact_df[~exact_df['id'].isin(not_available_imgs)]\n",
    "exact_df = exact_df.reset_index(drop=True)\n",
    "features = Features({\n",
    "    'image': Image(),\n",
    "    'labels': ClassLabel(names=exact_df['labels'].unique().tolist()),\n",
    "    'id': Value('string')\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_pandas(exact_df, features=features)\n",
    "# Access the ClassLabel feature\n",
    "class_label = features['labels']\n",
    "\n",
    "# Convert integer labels to string labels\n",
    "string_labels = [class_label.int2str(label) for label in dataset['labels']]\n",
    "# dataset = dataset.add_column('string_labels', string_labels)\n",
    "# ds = dataset.train_test_split(test_size = 0.1, stratify_by_column = 'labels', shuffle = True, seed = 42)\n",
    "# ds_train_val = ds['train'].train_test_split(test_size = 0.1, shuffle = True, stratify_by_column= 'labels', seed = 42)\n",
    "# ds = DatasetDict({\n",
    "#     'train': ds_train_val['train'],\n",
    "#     'val': ds_train_val['test'],\n",
    "#     'test': ds['test']})\n",
    "\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        T.CenterCrop(extractor.size[\"height\"]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/43974 [00:00<?, ? examples/s]c:\\Users\\sunny\\Desktop\\Home\\Projects\\Python\\Visual Search\\venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:252: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "Map: 100%|██████████| 43974/43974 [3:03:00<00:00,  4.00 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings(model: torch.nn.Module):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        image_batch_transformed = torch.stack(\n",
    "            [transformation_chain(image) for image in images]\n",
    "        )\n",
    "        new_batch = {\"pixel_values\": image_batch_transformed.to(device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "            del image_batch_transformed\n",
    "        return {\"embeddings\": embeddings}\n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "# Here, we map embedding extraction utility on our subset of candidate images.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extract_fn = extract_embeddings(model.to(device))\n",
    "dataset = load_from_disk(\"dataset\")\n",
    "candidate_subset_emb = dataset.map(extract_fn, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', np.array(candidate_subset_emb['embeddings']))\n",
    "\n",
    "# Save IDs\n",
    "np.save('ids.npy', np.array(candidate_subset_emb['id']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
